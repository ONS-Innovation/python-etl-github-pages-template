
"""ETL Pipeline runner script with command-line interface and optional MkDocs deployment."""

import argparse
import logging
import os
import sys
from pathlib import Path

from ${{ values.module_name }} import ETLPipeline

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_parser() -> argparse.ArgumentParser:
    """Create command-line argument parser."""
    parser = argparse.ArgumentParser(
        description="Run ETL Pipeline with optional MkDocs documentation deployment",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage
  python run_etl.py --source data.csv --output results/processed.csv

  # With transformations and documentation deployment
  python run_etl.py --source data.csv --output results/processed.csv --deploy

  # Specify custom site name and description
  python run_etl.py --source data.csv --output results/processed.csv --deploy \\
    --site-name "My Project Analysis" --site-description "ETL Results Documentation"
        """
    )

    # Required arguments
    parser.add_argument(
        '--source', '-s',
        required=True,
        help='Path to source data file'
    )
    parser.add_argument(
        '--output', '-o',
        required=True,
        help='Path for output data file'
    )

    # Optional arguments
    parser.add_argument(
        '--source-type',
        choices=['csv', 'xlsx', 'json'],
        default='csv',
        help='Type of source data (default: csv)'
    )
    parser.add_argument(
        '--format', '-f',
        choices=['csv', 'parquet', 'json'],
        default='csv',
        help='Output format (default: csv)'
    )
    parser.add_argument(
        '--no-transforms',
        action='store_true',
        help='Skip data transformations'
    )

    # Deployment options
{% if values.enable_deployment %}
    parser.add_argument(
        '--deploy',
        action='store_true',
        help='Deploy results as MkDocs documentation to GitHub Pages'
    )
    parser.add_argument(
        '--site-name',
        default=os.getenv('DOCS_SITE_NAME', '${{ values.docs_site_name or values.repository_name }}'),
        help='Documentation site name (default: from environment or template)'
    )
    parser.add_argument(
        '--site-description',
        default=os.getenv('DOCS_SITE_DESCRIPTION', '${{ values.docs_site_description or values.repository_description }}'),
        help='Documentation site description (default: from environment or template)'
    )
    parser.add_argument(
        '--project-name',
        default='${{ values.repository_name }}',
        help='Project name for documentation title'
    )
{% endif %}

    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Enable verbose logging'
    )

    return parser


def validate_arguments(args):
    """Validate command-line arguments."""
    # Validate source file exists
    if not Path(args.source).exists():
        logger.error(f"Source file not found: {args.source}")
        sys.exit(1)

    # Create output directory if it doesn't exist
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)


def configure_logging(verbose: bool):
    """Configure logging based on command-line arguments."""
    if verbose:
        logging.getLogger().setLevel(logging.DEBUG)


def prepare_pipeline_args(args):
    """Prepare arguments for the ETL pipeline."""
    pipeline_args = {
        'source_path': args.source,
        'output_path': args.output,
        'source_type': args.source_type,
        'output_format': args.format,
        'apply_transforms': not args.no_transforms,
    }

{% if values.enable_deployment %}
    # Add deployment arguments if enabled
    if args.deploy:
        pipeline_args.update({
            'enable_deployment': True,
            'site_name': args.site_name,
            'site_description': args.site_description,
            'project_name': args.project_name
        })

        logger.info(f"Documentation deployment enabled - Site: {args.site_name}")
{% endif %}

    return pipeline_args


def log_pipeline_start(args):
    """Log pipeline start information."""
    logger.info("Starting ETL pipeline...")
    logger.info(f"Source: {args.source}")
    logger.info(f"Output: {args.output}")
    logger.info(f"Transformations: {'Enabled' if not args.no_transforms else 'Disabled'}")


def display_pipeline_summary(pipeline):
    """Display pipeline execution summary."""
    summary = pipeline.get_pipeline_summary()
    logger.info("\nPipeline Summary:")
    logger.info(f"  Rows extracted: {summary['extract']['rows_extracted']}")

    if 'transform' in summary:
        logger.info(f"  Final rows after transform: {summary['transform']['final_rows']}")
        logger.info(f"  Transformations applied: {len(summary['transform']['transformations_applied'])}")

    if 'load' in summary:
        logger.info(f"  Output file: {summary['load']['output_path']}")

{% if values.enable_deployment %}
    if 'deploy' in summary and summary['deploy']['status'] == 'success':
        docs_url = summary['deploy']['docs_url']
        logger.info(f"  Documentation URL: {docs_url}")
        print(f"\nYour data documentation is available at: {docs_url}")
{% endif %}


def run_pipeline(pipeline_args, verbose=False):
    """Execute the ETL pipeline with error handling."""
    pipeline = ETLPipeline()

    try:
        success = pipeline.run_pipeline(**pipeline_args)
        return success, pipeline
    except Exception as e:
        logger.error(f"ETL pipeline failed with error: {e}")
        if verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


def start_etl():
    """Start the ETL pipeline with command-line interface."""
    parser = create_parser()
    args = parser.parse_args()

    # Configure logging level
    configure_logging(args.verbose)

    # Validate arguments
    validate_arguments(args)

    # Prepare pipeline arguments
    pipeline_args = prepare_pipeline_args(args)

    # Log pipeline start
    log_pipeline_start(args)

    # Run the pipeline
    success, pipeline = run_pipeline(pipeline_args, args.verbose)

    if success:
        logger.info("ETL pipeline completed successfully!")
        display_pipeline_summary(pipeline)
    else:
        logger.error("ETL pipeline failed!")
        sys.exit(1)


if __name__ == "__main__":
    start_etl()
