
"""ETL Pipeline runner script with command-line interface and optional S3 deployment."""

import argparse
import logging
import os
import sys
from pathlib import Path

from ${{ values.module_name }} import ETLPipeline

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_parser() -> argparse.ArgumentParser:
    """Create command-line argument parser."""
    parser = argparse.ArgumentParser(
        description="Run ETL Pipeline with optional S3 deployment",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage
  python run_etl.py --source data.csv --output results/processed.csv

  # With transformations and deployment
  python run_etl.py --source data.csv --output results/processed.csv --deploy

  # Specify S3 bucket and project name
  python run_etl.py --source data.csv --output results/processed.csv --deploy \\
    --s3-bucket my-bucket --project-name "My Project"
        """
    )

    # Required arguments
    parser.add_argument(
        '--source', '-s',
        required=True,
        help='Path to source data file'
    )
    parser.add_argument(
        '--output', '-o',
        required=True,
        help='Path for output data file'
    )

    # Optional arguments
    parser.add_argument(
        '--source-type',
        choices=['csv', 'xlsx', 'json'],
        default='csv',
        help='Type of source data (default: csv)'
    )
    parser.add_argument(
        '--format', '-f',
        choices=['csv', 'parquet', 'json'],
        default='csv',
        help='Output format (default: csv)'
    )
    parser.add_argument(
        '--no-transforms',
        action='store_true',
        help='Skip data transformations'
    )

    # Deployment options
{% if values.enable_s3_deployment %}
    parser.add_argument(
        '--deploy',
        action='store_true',
        help='Deploy results to S3 as static website'
    )
    parser.add_argument(
        '--s3-bucket',
        default=os.getenv('S3_BUCKET_NAME', '${{ values.s3_bucket_name }}'),
        help='S3 bucket name for deployment (default: from environment or template)'
    )
    parser.add_argument(
        '--aws-region',
        default=os.getenv('AWS_REGION', '${{ values.aws_region }}'),
        help='AWS region (default: from environment or template)'
    )
    parser.add_argument(
        '--project-name',
        default='${{ values.repository_name }}',
        help='Project name for website title and S3 prefix'
    )
{% endif %}

    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Enable verbose logging'
    )

    return parser


def validate_arguments(args):
    """Validate command-line arguments."""
    # Validate source file exists
    if not Path(args.source).exists():
        logger.error(f"Source file not found: {args.source}")
        sys.exit(1)

    # Create output directory if it doesn't exist
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)


def configure_logging(args):
    """Configure logging based on command-line arguments."""
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)


def prepare_pipeline_args(args):
    """Prepare arguments for the ETL pipeline."""
    pipeline_args = {
        'source_path': args.source,
        'output_path': args.output,
        'source_type': args.source_type,
        'output_format': args.format,
        'apply_transforms': not args.no_transforms,
    }

{% if values.enable_s3_deployment %}
    # Add deployment arguments if enabled
    if args.deploy:
        if not args.s3_bucket:
            logger.error("S3 bucket name required for deployment. Use --s3-bucket or set S3_BUCKET_NAME environment variable")
            sys.exit(1)

        pipeline_args.update({
            'enable_deployment': True,
            's3_bucket_name': args.s3_bucket,
            'project_name': args.project_name,
            'aws_region': args.aws_region
        })

        logger.info(f"Deployment enabled - S3 bucket: {args.s3_bucket}, Region: {args.aws_region}")
{% endif %}

    return pipeline_args


def log_pipeline_start(args):
    """Log pipeline start information."""
    logger.info("Starting ETL pipeline...")
    logger.info(f"Source: {args.source}")
    logger.info(f"Output: {args.output}")
    logger.info(f"Transformations: {'Enabled' if not args.no_transforms else 'Disabled'}")


def display_pipeline_summary(pipeline):
    """Display pipeline execution summary."""
    summary = pipeline.get_pipeline_summary()
    logger.info("\nPipeline Summary:")
    logger.info(f"  Rows extracted: {summary['extract']['rows_extracted']}")

    if 'transform' in summary:
        logger.info(f"  Final rows after transform: {summary['transform']['final_rows']}")
        logger.info(f"  Transformations applied: {len(summary['transform']['transformations_applied'])}")

    if 'load' in summary:
        logger.info(f"  Output file: {summary['load']['output_path']}")

{% if values.enable_s3_deployment %}
    if 'deploy' in summary and summary['deploy']['status'] == 'success':
        website_url = summary['deploy']['website_url']
        logger.info(f"  Website URL: {website_url}")
        print(f"\nYour data report is live at: {website_url}")
{% endif %}


def run_pipeline(pipeline_args):
    """Execute the ETL pipeline with error handling."""
    pipeline = ETLPipeline()

    try:
        success = pipeline.run_pipeline(**pipeline_args)
        return success, pipeline
    except Exception as e:
        logger.error(f"ETL pipeline failed with error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


def start_etl():
    """Start the ETL pipeline with command-line interface."""
    parser = create_parser()
    args = parser.parse_args()

    # Configure logging level
    configure_logging(args)

    # Validate arguments
    validate_arguments(args)

    # Prepare pipeline arguments
    pipeline_args = prepare_pipeline_args(args)

    # Log pipeline start
    log_pipeline_start(args)

    # Run the pipeline
    success, pipeline = run_pipeline(pipeline_args)

    if success:
        logger.info("ETL pipeline completed successfully!")
        display_pipeline_summary(pipeline)
    else:
        logger.error("ETL pipeline failed!")
        sys.exit(1)


if __name__ == "__main__":
    start_etl()
